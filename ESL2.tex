\documentclass{beamer}
\usetheme{Madrid}
\usepackage{amsmath}
\usepackage{bbm}
\usepackage{bm}
\usepackage{graphicx}
\usepackage{xcolor}
\usepackage[document]{ragged2e}
\usepackage{graphicx}
\usepackage{placeins}
\usepackage{float}
\usepackage{subfigure}
\usepackage{hyperref}
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,      
    urlcolor=cyan,
}
\usepackage[round]{natbib}

\newcommand{\E}{\mathbb{E}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\x}{\bm{x}}
\newcommand{\X}{\bm{X}}
\newcommand{\z}{\bm{z}}
\newcommand{\y}{\bm{y}}

\title{Elements of statistical learning:
Chapter 3}
\centering
\begin{document}
\maketitle

\begin{frame}{Content}
\tableofcontents
\end{frame}

\section{Linear models}

\begin{frame}{LS estimator}
Let $\bm{X}$ be an $N\times(p+1)$ matrix of explanatory variables and $\bm{y}$ an $N\times 1$ vector of outputs. Then we know the LS estimator $\hat{\beta}$
\[
\hat{\beta}=(\bm{X}'\bm{X})^{-1}\bm{X}'\bm{y},
\]
 [see lecture slides "ESL1" for recap and proof]. 
\begin{block}{The "hat" matrix}
As such for the fitted linear model
\begin{eqnarray*}
\hat{y}&=&\bm{X}\hat{\beta}\\
&=&\underbrace{\bm{X}(\bm{X}'\bm{X})^{-1}\bm{X}'}_{\bm{H}}\bm{y}\\
&=&\bm{H}\bm{y}
\end{eqnarray*}
where $\bm{H}$ is commonly referred to as the hat matrix.
\end{block}
\end{frame}

\begin{frame}{$\bm{H}$ the projection matrix}
Let us denote the column vectors of $\bm{X}$ by $\bm{x}_0,\bm{x}_1,\cdots,\bm{x}_p$ with $\bm{x}_0\equiv 1$.
\begin{itemize}
\item{} These vectors span a subspace of $\R^N$, also referred to as a column vector of $\bm{X}$.
\item{} We minimize $RSS(\beta)=\lvert\lvert \bm{y}-\bm{X}\beta\rvert\rvert^2$ by choosing $\hat{\beta}$, so that the residual vector $\bm{y}-\hat{\bm{y}}$ is orthogonal to this subspace.
\item{} the hat matrix $\bm{H}$ computes the orthogonal projection, and hence it is also known as the projection matrix.
\end{itemize}
\end{frame}

\subsection{Sampling properties of $\hat{\beta}$}
\begin{frame}{Variance-covariance matrix}
\begin{block}{Assumptions}
\begin{enumerate}
\item{} Observations $y_i$ are uncorrelated have constant variance $\sigma^2$
\item{} $x_i$ are fixed (i.e. non-stochastic)
\end{enumerate}
\end{block}
\begin{eqnarray*}
var(\hat{\beta})&=&var\left[(\bm{X}'\bm{X})^{-1}\bm{X}'\bm{y}\right]\\
&=&var\left[(\bm{X}'\bm{X})^{-1}\bm{X}'(\bm{X}\beta+\epsilon)\right]\\
&=&var\left[(\bm{X}'\bm{X})^{-1}(\bm{X}'\bm{X})\beta+(\bm{X}'\bm{X})^{-1}\bm{X}'\bm{\epsilon}\right]\\
&=&var\left[(\bm{X}'\bm{X})^{-1}\bm{X}'\bm{\epsilon}\right]\\
&=&\E\left\{(\bm{X}'\bm{X})^{-1}\bm{X}'\bm{\epsilon}[(\bm{X}'\bm{X})^{-1}\bm{X}'\bm{\epsilon}]'\right\}\\
&=&\E\left\{(\bm{X}'\bm{X})^{-1}\bm{X}'\bm{\epsilon}\bm{\epsilon}'\bm{X}(\bm{X}'\bm{X})^{-1}\right\}\\
&=&\E\left\{(\bm{X}'\bm{X})^{-1}(\bm{X}'\bm{X})\bm{\epsilon}\bm{\epsilon}'(\bm{X}'\bm{X})^{-1}\right\}
\end{eqnarray*}
\end{frame}

\begin{frame}
Note that $\epsilon$ is the error term and has zero mean and also remember that $\bm{X}$ is fixed, and thus 
\[
\E[aZ]=a\E[Z]
\]
where $Z$ is a random variable and $a$ is a constant. Therefore, 
\begin{eqnarray*}
var(\hat{\beta})&=&\E\left\{\bm{\epsilon}\bm{\epsilon}'(\bm{X}'\bm{X})^{-1}\right\}\\
&=&(\bm{X}'\bm{X})^{-1}E\left\{\bm{\epsilon}\bm{\epsilon}'\right\}\\
&=&(\bm{X}'\bm{X})^{-1}\sigma^2
\end{eqnarray*}
where $\sigma^2$ can be calculated by
\[
\sigma^2=\frac{1}{N-p-1}\sum\limits_{i=1}^N(y_i-\hat{y}_i)^2
\]
thus, assuming the errors are further Gaussian
\[
\hat{\beta}\sim N(\beta,(\bm{X}'\bm{X})^{-1}\sigma^2)
\]
\end{frame}
\subsection{Gauss-Markov Theorem}
\begin{frame}{Gauss-Markov Theorem}
Least squares estimator of parameter $\beta$ has the smallest variance among all linear unbiased estimators.
Why is the LS estimator unbiased?
\begin{proof}
\begin{eqnarray*}
\hat{\beta}&=&\E[\hat{\beta}]\\
&=&\E[(\bm{X}'\bm{X})^{-1}\bm{X}'\bm{y}]\\
&=&\E[(\bm{X}'\bm{X})^{-1}\bm{X}'(\bm{X}\beta+\bm{\epsilon})]\\
&=&\E[\beta+(\bm{X}'\bm{X})^{-1}\bm{X}'\bm{\epsilon}]\\
&=&\beta+(\bm{X}'\bm{X})^{-1}\bm{X}'\E[\bm{\epsilon}]\\
&=&\beta
\end{eqnarray*}
\end{proof}
\end{frame}

\section{Multiple regression}
\subsection{From simple univariate to multiple regressions}
\begin{frame}{From simple univariate to muliple regressions}
Suppose first we have a univariate model with no intercept
\[
Y_i=X_i\beta+\varepsilon_i
\]
The least squares estimates and residuals are
\[
\hat{\beta}=\frac{\sum\limits_{i=1}^{N}x_iy_i}{\sum\limits_{i=1}^{N}x_i^2}
\]
with residuals
\[
r_i=y_i-x_i\hat{\beta}
\]
which in vector notation can be expressed as 
\[
<\bm{x},\bm{y}>=\sum\limits_{i=1}^{N}x_iy_i=\bm{x}'\bm{y}
\] 
which is the inner product between $\bm{x}$ and $\bm{y}$.
\end{frame}

\begin{frame}
Thus, the OLS estimator $\hat{\beta}$ can be expressed as follows
\[
\hat{\beta}=\frac{<\bm{x},\bm{y}>}{<\bm{x},\bm{x}>},
\]
Suppose now that we have $p$ inputs $\bm{x}_1,\cdots,\bm{x}_p$, which are the columns of the matrix $\bm{X}$ and are orthogonal, such that $<\bm{x}_j,\bm{x}_k>=0$ for all $i\neq j$. When the inputs are orthogonal, the multiple least squares estimates $\hat{\beta}_j$ are equal tothe univariate estimates - i.e. 
\[
\hat{\beta}_j=\frac{<\bm{x}_j,\bm{y}>}{<\bm{x}_j,\bm{x}_j>}
\] 
In other words, the inputs are orthogonal and have no impact on each other's parameters estimates in the model.
\end{frame}

\begin{frame}
Consider the case of an intercept and a single input $\bm{x}$, then the least squares coefficient of $\bm{x}$ has the form
\[
\hat{\beta}_1=\frac{<\bm{x}-\bar{x}\bm{1},\bm{y}>}{<\bm{x}-\bar{x}\bm{1},\bm{x}-\bar{x}\bm{1}>}
\]
The steps of the algorithm can be seen as follows
\begin{enumerate}
\item{} Regress $\bm{x}$ on $\bm{1}$ to obtain $\bar{x}\bm{1}$

\item{} Obtain the residuals $\bm{z}=\bm{x}-\bar{x}\bm{1}$

\item{} Regress $\bm{y}$ on $\bm{z}$ to obtain the coefficient $\hat{\beta}_1$
\end{enumerate}
\[
\hat{\beta}_1=\frac{<\bm{z},\bm{y}>}{<\bm{z},\bm{z}>}
\]
Step 1 orthogonalizes $x$ with respect to $\bm{x}_0=1$.  
\end{frame}
\subsection{The Gram-Schmidt algorithm}
\begin{frame}{The Gram-Schmidt algorithm}
The idea is similar in the presence of more predictors. In the case of two predictors and an intercept, say, $\bm{x}_0=1, \bm{x}_1,\bm{x}_2$.
\begin{enumerate}
 \item{} First regress $\bm{x}_1$ on $\bm{x}_0=1$ and obtain the residual vector $\bm{z}_1=\bm{x}_1-\bar{x}\bm{1}$

\item{} Then regress $\bm{x}_2$ on $\x_0=1$ and $\z_1$ to produce the coefficients $\hat{\gamma}_1$ and obtain the residual vector $\z_2=\x_2-\bar{x}1-\hat{\gamma}_1\z_1$

\item{} Regress $\y$ on the residual $\z_p$ to get the estimate $\hat{\beta}_p$.
\end{enumerate}
This algorithm can alternatively be expressed in matrix format. In other words, the second step can be written as follows
\[
\X=\bm{Z}\bm{\Gamma}
\]
(\textbf{Note:} For a review of QR decomposition and its application to Gram-Schmidt algorithm, \href{http://www.seas.ucla.edu/~vandenbe/133A/lectures/qr.pdf}{click here})
\end{frame}

\begin{frame}
with 
\[
\bm{Z}=
\begin{bmatrix}
1 & z_{11} & \cdots & z_{1p}\\
1& z_{21} &\cdots&z_{2p}\\
\vdots&\vdots&\ddots&\vdots\\
1& z_{N1}&\cdots &z_{Np}
\end{bmatrix}\quad\text{and}\quad
\bm{\Gamma}=
\begin{bmatrix}
\bar{x}&\bar{x}&\bar{x}&\cdots&\bar{x}\\
&\hat{\gamma}_{1}&\hat{\gamma}_{1}&\cdots&\hat{\gamma}_1\\
&&\hat{\gamma}_{2}&\cdots& \hat{\gamma}_2\\
&&&\ddots&\vdots\\
0&&&&\hat{\gamma}_p
\end{bmatrix}
\]
we then introduce a diagonal matrix $\bm{D}$ with $j^{\text{th}}$ diagonal entry $D_{jj}=\lvert\lvert z_j\rvert\rvert$, - i.e. 
\[
\bm{D}=
\begin{bmatrix}
\lvert\lvert z_0\rvert\rvert&0&\cdots&0\\
0&\lvert\lvert z_1\rvert\rvert&\cdots&0\\
\vdots&\vdots&\ddots&\vdots\\
0&0&\cdots&\lvert\lvert z_p\rvert\rvert
\end{bmatrix}
\]
and we express the matrix $X$ as follows
\[
\X=\bm{Z}\bm{D}^{-1}\bm{D}\bm{\Gamma}
\]
\end{frame}
\begin{frame}
Noting that $\bm{Q}=\bm{Z}\bm{D}^{-1}$ is $N\times (p+1)$ with orthonormal columns, - i.e. $\bm{Q}'\bm{Q}=\bm{I}$ and $\bm{D\Gamma}$ is a $(p+1)\times(p+1)$ upper triangular matrix. 
Thus, the least squares estimator is given by
\begin{eqnarray*}
\hat{\beta}=(\X'\X)^{-1}\X'\bm{y}&=&[(\bm{Q}\bm{R})'(\bm{Q}\bm{R})]^{-1}\bm{Q}'\bm{R}\bm{y}\\
&=&[\bm{R}'\bm{Q}'\bm{Q}\bm{R}]^{-1}\bm{Q}'\bm{R}\bm{y}\\
&=&[\bm{R}'\bm{I}\bm{R}]^{-1}\bm{Q}'\bm{R}\bm{y}\\
&=&\bm{R}^{-1}\bm{Q}'y
\end{eqnarray*}
similarly,
\begin{eqnarray*}
\hat{y}&=&X\hat{\beta}\\
&=&(\bm{QR})(\bm{R}^{-1}\bm{Q}'y)\\
&=&\bm{Q}\bm{Q}'\bm{y}
\end{eqnarray*}
\end{frame}
\section{Shrinkage methods}
\subsection{Ridge regression}
\begin{frame}{Ridge regression}
\begin{block}{Shrinkage methods}
Shrinkage methods shrink the regression coefficients by imposing a penalty on their size. The most notable shrinkage methods are the \textit{Lasso} and \textit{Ridge regressions}.
\end{block}
The ridge coefficients minimize a penalized residual sum of squares:
\begin{equation}\label{eq: ridge}
\hat{\beta}_{Ridge}=\arg\min_{\beta}\left\{\sum\limits_{i=1}^{N}(y_i-\beta_0-\sum\limits_{j=1}^{p}x_{ij}\beta_j)^2+\lambda\sum\limits_{j=1}^{p}\beta_j^2)\right\}
\end{equation}
Here $\lambda\geq0$ is a complexity parameter that controls the amount of shrinkage.
\end{frame}
\begin{frame}
For reasons outlined in age 65 of the book, let us centre the inpute $x_{ij}$ by $x_{ij}-\bar{x}_j$ and we estimate $\beta_0$ by $\bar{y}=\frac{1}{N}\sum\limits_{i=1}^{N}y_i$. Thus, the remaining coefficients gets estimated by a ridge regression without an intercept, where $\X$ has $p$ instead of $(p+1)$ columns. Henceforth, rekationship (\ref{eq: ridge}) can instead be expressed in the matrix format as follows
\[
\text{RSS}(\lambda)=(\y-\X\beta)'(\y-\X\beta)+\lambda\beta'\beta
\]
Thus, the solution to ridge regression can easily be seen
\begin{eqnarray*}
\hat{\beta}_{Ridge}&=&\arg\min_{\beta}\left\{(\y-\X\beta)'(\y-\X\beta)+\lambda\beta'\beta\right\}\\
&=&\arg\min_{\beta}\left\{(\y'-\beta'\X')(\y-\X\beta)+\lambda\beta'\beta\right\}\\
&=&\arg\min_{\beta}\left\{(\y'\y-\y'\X\beta-\beta'\X'\y+\beta'\X'\X\beta)+\lambda\beta'\beta\right\}\\
&=&-\X'\y-\X'\y+2\beta\X'\X+2\lambda\beta=0\\
&=&\beta(\X'\X+\lambda\bm{I})=\X'\y\\
&=&(\X'\X+\lambda\bm{I})^{-1}\X'\y
\end{eqnarray*}
\end{frame}
\subsection{SVD of ridge regression}
\begin{frame}{SVD of ridge regression}
The SVD of the $N\times p$ matrix $\X$ has the form
\[
\X=\bm{U}_{N\times p}\bm{D}_{p\times p}\bm{V}_{p\times p}'
\]
where $\bm{U}$ and $\bm{V}$ are orthogonal - i.e. 
\[
\bm{U}'\bm{U}=\bm{I},\quad \bm{V}'\bm{V}=\bm{I}
\]
(For a quick review of SVD, \href{https://mathworld.wolfram.com/SingularValueDecomposition.html}{click here}.)

Using the singular value decomposition, we can write the least squares fitted vector as 
\begin{eqnarray*}
\hat{y}=\X\hat{\beta}_{ls}&=&\X(\X'\X)^{-1}\X'\y\\
&=&\bm{UDV}[(\bm{UDV})'\bm{UDV})]^{-1}(\bm{UDV})'\y\\
&=&\bm{UDV}[\bm{V}'\bm{D}'\bm{U}'\bm{UDV}]^{-1}\bm{V}'\bm{D}'\bm{U}'\y\\
&=&\bm{UDV}[\bm{D}'\bm{D}]^{-1}\bm{V'}\bm{D'}\bm{U'}\y\\
&=&\bm{UU}'\y
\end{eqnarray*}
\end{frame}

\begin{frame}
Now for the ridge regression, we would have
\begin{eqnarray*}
\hat{y}=\X\hat{\beta}_{ridge}&=&\X(\X'\X+\lambda\bm{I})^{-1}\X'\y\\
&=&\bm{UDV}[(\bm{UDV})'\bm{UDV}+\lambda\bm{I}]^{-1}(\bm{UDV})'\y\\
&=&\bm{UDV}[\bm{V}'\bm{D}'\bm{U}'\bm{UDV}+\lambda\bm{I}]^{-1}\bm{V}'\bm{D}'\bm{U}'\y\\
&=&\bm{U}\bm{D}[\bm{D}'\bm{D}+\lambda\bm{I}]^{-1}\bm{D}'\bm{U}\y
\end{eqnarray*}
\end{frame}
%\bibliographystyle{apa}
%\bibliography{References_Final}
\end{document}
